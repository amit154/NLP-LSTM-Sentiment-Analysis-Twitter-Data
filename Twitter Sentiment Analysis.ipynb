{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np \nimport tensorflow as tf\nimport pandas as pd \nimport nltk\nimport os\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom bs4 import BeautifulSoup\nimport re\nfrom keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nimport random\nfrom tensorflow import set_random_seed\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense,Dropout,Embedding,LSTM\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.callbacks import EarlyStopping\nfrom keras.losses import categorical_crossentropy\nfrom keras.optimizers import Adam,SGD\n\nfrom keras.models import Sequential\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\nlemmatizer = WordNetLemmatizer()\nset_random_seed(123)\nrandom.seed(123)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"Using TensorFlow backend.\n","output_type":"stream"}]},{"cell_type":"code","source":"DATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\ndataset_filename = os.listdir(\"../input\")[0]\ndataset_path = os.path.join(\"..\",\"input\",dataset_filename)\nprint(\"Open file:\", dataset_path)\ntrain = pd.read_csv(dataset_path, encoding =DATASET_ENCODING , names=DATASET_COLUMNS)\ntrain.head()\n","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Open file: ../input/training.1600000.processed.noemoticon.csv\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"   target                        ...                                                                       text\n0       0                        ...                          @switchfoot http://twitpic.com/2y1zl - Awww, t...\n1       0                        ...                          is upset that he can't update his Facebook by ...\n2       0                        ...                          @Kenichan I dived many times for the ball. Man...\n3       0                        ...                            my whole body feels itchy and like its on fire \n4       0                        ...                          @nationwideclass no, it's not behaving at all....\n\n[5 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>target</th>\n      <th>ids</th>\n      <th>date</th>\n      <th>flag</th>\n      <th>user</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1467810369</td>\n      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>_TheSpecialOne_</td>\n      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>1467810672</td>\n      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>scotthamilton</td>\n      <td>is upset that he can't update his Facebook by ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1467810917</td>\n      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>mattycus</td>\n      <td>@Kenichan I dived many times for the ball. Man...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1467811184</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>ElleCTF</td>\n      <td>my whole body feels itchy and like its on fire</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>1467811193</td>\n      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n      <td>NO_QUERY</td>\n      <td>Karoli</td>\n      <td>@nationwideclass no, it's not behaving at all....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def clean_sentences(df):\n    df['review_text'] = df['text'].apply(lambda x:re.sub(\"[^a-zA-Z]\",\" \", x))\n    df['review_text'] = df['review_text'].apply(lambda x:word_tokenize(x.lower()))\n    df['review_text'] = df['review_text'].apply(lambda x:[lemmatizer.lemmatize(i) for i in x])\n    return(list(df['review_text']))\n\n#cleaned reviews for both train and test set retrieved\ntrain_sentences = clean_sentences(train)","metadata":{"_uuid":"921083f2a081dddeafadee9264974fa41bd27c0f","trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train.target.tolist())\ny_target = encoder.transform(train.target.tolist())\ny_target = y_target.reshape(-1,1)\n\n\nx_train, x_test, y_train, y_test = train_test_split(train_sentences, y_target, test_size=0.25, random_state=2)","metadata":{"_uuid":"217da6b602ad02c60cc3cccc79efd7b703f00087","trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"unique_words = set()\nlen_max = 0\n\nfor sent in tqdm(x_train):\n    \n    unique_words.update(sent)\n    \n    if(len_max<len(sent)):\n        len_max = len(sent)\n        \n#length of the list of unique_words gives the no of unique words\nprint(len(list(unique_words)))\nprint(len_max)","metadata":{"_uuid":"b43811cb801379a5cdb8aaa3837a6e2652eaab52","trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 1200000/1200000 [00:02<00:00, 437291.38it/s]","output_type":"stream"},{"name":"stdout","text":"476286\n53\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=len(list(unique_words)))\ntokenizer.fit_on_texts(list(x_train))\nx_train = tokenizer.texts_to_sequences(x_train)\nx_test = tokenizer.texts_to_sequences(x_test)\n\n\n#padding done to equalize the lengths of all input reviews. LSTM networks needs all inputs to be same length.\n#Therefore reviews lesser than max length will be made equal using extra zeros at end. This is padding.\nx_train = sequence.pad_sequences(x_train, maxlen=len_max)\nx_test = sequence.pad_sequences(x_test, maxlen=len_max)\nprint(x_train.shape,x_test.shape)","metadata":{"_uuid":"b821bf6f4aaf40f4a31cfe8425e8b24cf8a7cf55","trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(1200000, 53) (400000, 53)\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping = EarlyStopping(min_delta = 0.001, mode = 'max', monitor='val_acc', patience = 2)\ncallback = [early_stopping]\n\n#Model using Keras LSTM\nmodel=Sequential()\nmodel.add(Embedding(len(list(unique_words)),300,input_length=len_max))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(128, dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer=Adam(lr=0.005),metrics=['accuracy'])\nmodel.summary()","metadata":{"_uuid":"3a3d8030794272d3cff5e573b36515626be6d657","trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 53, 300)           142885800 \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 53, 300)           0         \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 128)               219648    \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 143,105,577\nTrainable params: 143,105,577\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"# callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n#               EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","metadata":{"_uuid":"f1cc6f9a28abe35abdc54c1a321645f933182758","trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"history = model.fit(x_train, y_train,\n                    batch_size=1024,\n                    epochs=1,\n                    validation_split=0.2,\n                    verbose=1)\n(loss, accuracy) =model.evaluate(x_test, y_test, batch_size=1024, verbose=1)\nprint(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss,accuracy * 100))\nmodel.save('my_sentiment_model.h5') ","metadata":{"_uuid":"05729b4e6111314e91a874e48d5b2ec566d3981f","trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 142885800 elements. This may consume a large amount of memory.\n  num_elements)\n","output_type":"stream"},{"name":"stdout","text":"Train on 960000 samples, validate on 240000 samples\nEpoch 1/1\n960000/960000 [==============================] - 143s 149us/step - loss: 0.4147 - acc: 0.8088 - val_loss: 0.3805 - val_acc: 0.8290\n400000/400000 [==============================] - 12s 30us/step\n[INFO] loss=0.3843, accuracy: 82.6352%\n","output_type":"stream"}]},{"cell_type":"code","source":"SENTIMENT_THRESHOLDS=np.array([0.4,0.7])\ndef decode_sentiment(score, include_neutral=True):\n    if include_neutral:        \n        label = \"NEUTRAL\"\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = \"NEGATIVE\"\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = \"POSITIVE\"\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","metadata":{"_uuid":"fba72d77032bca00627acccf0d8391839b4972bb","trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def predict(text, include_neutral=True):\n    \n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=len_max)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score)\n       } ","metadata":{"_uuid":"6a0bf7673817cd8ff4f8404eac848525c78c444a","trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(predict(\"I am so happy\"))\nprint(predict(\"Oh! No\"))\nprint(predict(\"I am going out\"))","metadata":{"_uuid":"71392a081378d6fb6248775afc6cb0daf25fc2cb","trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"{'label': 'POSITIVE', 'score': 0.9787561893463135}\n{'label': 'NEGATIVE', 'score': 0.042689982801675797}\n{'label': 'NEUTRAL', 'score': 0.6324208378791809}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}